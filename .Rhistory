smoothsplineplot<-function(x,y,cv=FALSE){
fit=smooth.spline(x,y,cv)
xp=seq(min(X),max(X),length=101)
plot(X,Y,col="black")
lines(fit$x,fit$y,col="red")
if(!cv)
legend=c("generalized cross-validation")else
legend=c("leave-one-out")
legend("bottomright",inset=.01,legend=legend,lty=1,col="red",horiz=FALSE,cex=0.7)
}
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
set.seed(123)
f<-function(x){sin(12*(x+0.2))/(x+0.2)}
N=100
X=runif(N)
e=rnorm(N)
Y=f(X)+e
smoothsplineplot(X,Y)
smoothsplineplot<-function(x,y,cv=FALSE){
fit=smooth.spline(x,y,cv)
xp=seq(min(X),max(X),length=101)
plot(X,Y,col="black")
lines(fit$x,fit$y,col="red")
if(!cv)
legend=c("generalized cross-validation")else
legend=c("leave-one-out")
legend("bottomright",inset=.01,legend=legend,lty=1,col="red",horiz=FALSE,cex=0.7)
}
f<-function(x){sin(2X)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
f<-function(x){sin(2*X)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
f<-function(x){sin(2*x)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
smoothsplineplot(x,y)
name=c("generalized cross-validation")
name
class(name)
length(name)
smoothsplineplot<-function(x,y,cv=FALSE){
fit=smooth.spline(x,y,cv)
xp=seq(min(X),max(X),length=101)
plot(X,Y,col="black")
lines(fit$x,fit$y,col="red")
if(!cv)
name=c("generalized cross-validation")else
name=c("leave-one-out")
legend("bottomright",inset=.01,legend=legend,lty=1,col="red",horiz=FALSE,cex=0.7)
}
f<-function(x){sin(2*x)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
smoothsplineplot(x,y)
smoothsplineplot<-function(x,y,cv=FALSE){
fit=smooth.spline(x,y,cv)
xp=seq(min(x),max(x),length=101)
plot(x,y,col="black")
lines(fit$x,fit$y,col="red")
if(!cv)
name=c("generalized cross-validation")else
name=c("leave-one-out")
legend("bottomright",inset=.01,legend=legend,lty=1,col="red",horiz=FALSE,cex=0.7)
}
f<-function(x){sin(2*x)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
smoothsplineplot(x,y)
smoothsplineplot<-function(x,y,cv=FALSE){
fit=smooth.spline(x,y,cv)
xp=seq(min(x),max(x),length=101)
plot(x,y,col="black")
lines(fit$x,fit$y,col="red")
#if(!cv)
# name=c("generalized cross-validation")else
# name=c("leave-one-out")
#legend("bottomright",inset=.01,legend=legend,lty=1,col="red",horiz=FALSE,cex=0.7)
}
f<-function(x){sin(2*x)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
smoothsplineplot(x,y)
f<-function(x){sin(2*x)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
x
y
fit=smooth.spline(x,y,cv)
fit=smooth.spline(x,y,cv=FALSE)
xp=seq(min(x),max(x),length=101)
plot(x,y,col="black")
lines(fit$x,fit$y,col="red")
f<-function(x){sin(12*(x+0.2))/(x+0.2)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
fit=smooth.spline(x,y,cv=FALSE)
xp=seq(min(x),max(x),length=101)
plot(x,y,col="black")
lines(fit$x,fit$y,col="red")
smoothsplineplot<-function(x,y,cv=FALSE){
fit=smooth.spline(x,y,cv=FALSE)
xp=seq(min(x),max(x),length=101)
plot(x,y,col="black")
lines(fit$x,fit$y,col="red")
#if(!cv)
# name=c("generalized cross-validation")else
# name=c("leave-one-out")
#legend("bottomright",inset=.01,legend=legend,lty=1,col="red",horiz=FALSE,cex=0.7)
}
f<-function(x){sin(12*(x+0.2))/(x+0.2)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
smoothsplineplot(x,y)
smoothsplineplot<-function(x,y,cv=FALSE){
fit=smooth.spline(x,y,cv=FALSE)
xp=seq(min(x),max(x),length=101)
plot(x,y,col="black")
lines(fit$x,fit$y,col="red")
if(!cv)
name=c("generalized cross-validation")else
name=c("leave-one-out")
legend("bottomright",inset=.01,legend=legend,lty=1,col="red",horiz=FALSE,cex=0.7)
}
f<-function(x){sin(12*(x+0.2))/(x+0.2)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
smoothsplineplot(x,y)
smoothsplineplot<-function(x,y,cv=FALSE){
fit=smooth.spline(x,y,cv=FALSE)
xp=seq(min(x),max(x),length=101)
plot(x,y,col="black")
lines(fit$x,fit$y,col="red")
if(!cv)
name=c("generalized cross-validation")else
name=c("leave-one-out")
legend("bottomright",inset=.1,legend=legend,lty=1,col="red",horiz=FALSE)
}
f<-function(x){sin(12*(x+0.2))/(x+0.2)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
smoothsplineplot(x,y)
smoothsplineplot<-function(x,y,cv=FALSE){
fit=smooth.spline(x,y,cv=FALSE)
xp=seq(min(x),max(x),length=101)
plot(x,y,col="black")
lines(fit$x,fit$y,col="red")
if(!cv)
name=c("generalized cross-validation")else
name=c("leave-one-out")
legend("bottomright",inset=.1,legend=name,lty=1,col="red",horiz=FALSE)
}
f<-function(x){sin(12*(x+0.2))/(x+0.2)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
smoothsplineplot(x,y)
smoothsplineplot<-function(x,y,cv=FALSE){
fit=smooth.spline(x,y,cv=FALSE)
xp=seq(min(x),max(x),length=101)
plot(x,y,col="black")
lines(fit$x,fit$y,col="red")
if(!cv)
name=c("generalized cross-validation")else
name=c("leave-one-out")
legend("bottomright",inset=.01,legend=name,lty=1,col="red",horiz=FALSE,cex=0.7)
}
f<-function(x){sin(12*(x+0.2))/(x+0.2)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
smoothsplineplot(x,y)
smoothsplineplot<-function(x,y,cv=FALSE){
fit=smooth.spline(x,y,cv=FALSE)
xp=seq(min(x),max(x),length=101)
plot(x,y,col="black")
lines(fit$x,fit$y,col="red")
if(!cv)
name=c("generalized cross-validation")else
name=c("leave-one-out")
legend("bottomright",inset=.01,legend=name,lty=1,col="red",horiz=FALSE,cex=0.5)
}
f<-function(x){sin(12*(x+0.2))/(x+0.2)}
N=100
x=runif(N)
e=rnorm(N)
y=f(x)+e
smoothsplineplot(x,y)
smoothsplineplot(x,y,cv=TRUE)
smoothsplineplot(x,y)
smoothsplineplot(x,y,cv=TRUE)
set.seed(123)
f<-function(x){sin(12*(x+0.2))/(x+0.2)}
N=100
X=runif(N)
e=rnorm(N)
Y=f(X)+e
fit1=smooth.spline(X,Y,cv=FALSE) #‘generalized’ cross-validation when cv=FALSE
fit2=smooth.spline(X,Y,cv=TRUE) #ordinary leave-one-out when cv=TRUE
xp=seq(min(X),max(X),length=101)
plot(X,Y,col="black",ylim=c(-3,4))
lines(xp,f(xp),lw=2,col=1)
lines(fit1$x,fit1$y,lwd=5,col=2)
points(fit2$x,fit2$y,lwd=2,col=3,type="l")
legend("bottomright",inset=.01,legend=c("TRUE","generalized cross-validation","leave-one-out"),lty=1,col=1:3,horiz=FALSE,cex=0.7)
fit1$spar
fit2$spar
X=X[order(X)]
Y=Y[order(X)]
df=c(5,9,15)
alpha = 0.05
getSmootherMat = function(x, df) {
n = length(x)
S = matrix(0,n,n)
for (i in 1:n) {
if (i%%100==0) cat(i,"... ")
y = numeric(n)
y[i] = 1
S[,i] = predict(smooth.spline(x,y,df=df),x)$y
}
return(S)
}
for(i in 1:3){
S=getSmootherMat(X,df=df[i])
ssmod=smooth.spline(X,Y,df=df[i])
yhat = predict(ssmod,X)$y
sigmahat = sqrt(sum((Y-yhat)^2)/(N-df[i]))
syhat = sigmahat * sqrt(diag(S%*%t(S)))
q1 = qt(alpha/2,N-df[i])
q2 = qt(1-alpha/2,N-df[i])
low.lims = yhat-syhat*q2
up.lims = yhat-syhat*q1
plot(X,Y,col="black",ylim=c(-3,4))
title(paste('df=',df[i]))
lines(xp,f(xp),lw=2,col=1)
lines(ssmod$x,ssmod$y,col=2)
points(X,low.lims,col=3,lty=2,type="l")
points(X,up.lims,col=3,lty=2,type="l")
legend("bottomright",inset=.01,legend=c("TRUE","spline","pointwise confidence intervals"),lty=1,col=1:3,horiz=FALSE,cex=0.7)
}
S
View(S)
numeric(100)
betacdf<-function(a,b,x){
m=1000000
u=runif(m/2,min=0,max=x)
u1=1-u
g=1/beta(a,b)*u^(a-1)*(1-u)^(b-1)*x
g1=1/beta(a,b)*u1^(a-1)*(1-u1)^(b-1)*x
theta=(mean(g)+mean(g1))/2
return(theta)
}
betacdf(2,3,0.5)
pbeta(0.5,2,3)
betacdf(3,3,0.5)
betacdf(3,3,0.3)
pbeta(0,3,3,3)
pbeta(0.3,3,3)
x=seq(0,1,0.1)
x
betacdf(3,3,x)
Rayleighsample<-function(n,sigma,antithetic=TRUE){
u=runif(n/2)
if(!antithetic)
{v=runif(n/2)}else
{v=1-u}
u=c(u,v)
x=sqrt(-2*sigma^2*log(1-u))
return(x)
}
Rayleighsample(1000,2)
Rayleighsample(1000,2)
sample=Rayleighsample(1000,2,antithetic=FALSE)
plot(sample)
betasample<-function(n,a,b){
m=(1-a)/(2-a-b)
max=m^(a-1)*((1-m)^(b-1))/beta(a,b)#maximum of f(x)
c<-max+3
j<-k<-0
y<-numeric(n)
while (k < n) {
u <- runif(1)
j <- j + 1
x <- runif(1)
rho<-x^(a-1)*((1-x)^(b-1))/beta(a,b)/c
if (rho> u) {
k <- k + 1
y[k] <- x
}
}
return(y)
}
sample=betasample(1000,3,2)
plot(sample)
#' @title distribution function for the Beta distribution
#' @description a function to compute a Monte Carlo estimate of the Beta cdf Using antithetic variables method
#' @param a,b non-negative parameters of the Beta distribution
#' @param x the value of quantile
#' @return the value of the Beta cdf at x
#' @examples
#' \dontrun{
#' betacdf(3,3,0.3)
#' }
#' @export
betacdf<-function(a,b,x){
m=1000000
u=runif(m/2,min=0,max=x)
u1=1-u
g=1/beta(a,b)*u^(a-1)*(1-u)^(b-1)*x
g1=1/beta(a,b)*u1^(a-1)*(1-u1)^(b-1)*x
theta=(mean(g)+mean(g1))/2
return(theta)
}
devtools::document()
library(StatComp18054)
ricker <- function(nzero, r, K=1, time=100, from=0, to=time) {
N <- numeric(time+1)
N[1] <- nzero
for (i in 1:time) N[i+1] <- N[i]*exp(r*(1 - N[i]/K))
Time <- 0:time
plot(Time, N, type="l", xlim=c(from, to))
}
layout(matrix(1:3, 3, 1))
ricker(0.1, 1); title("r = 1")
ricker(0.1, 2); title("r = 2")
ricker(0.1, 3); title("r = 3")
library(knitr)
knit(intro.Rmd)
m1<-matrix(1,nr=2,nc=2)
m2<-matrix(2,nr=2,nc=2)
# Merge vector or matrix
rbind(m1,m2)
cbind(m1,m2)
# Matrix product
rbind(m1,m2)%*%cbind(m1,m2)
cbind(m1,m2)%*%rbind(m1,m2)
# Diagonal operation and diagonal matrix
diag(m1)
diag(rbind(m1,m2)%*%cbind(m1,m2))
diag(m1)<-10
m1
diag(3)
v<-c(10,20,30)
diag(v)
diag(2.1,nr=3,nc=5)
# Inverse matrix
solve(diag(v))
# Matrix QR decomposition
qr(diag(v))
# Eigenvalue and eigenvector
eigen(diag(v))
# Use table to output eigenvectors
knitr::kable(eigen(diag(v))$vectors,col.names=c("first eigenvector", "second eigenvector", "third eigenvector"), align = c('c','c','c'))
# Singular value decomposition
svd(diag(v))
layout(matrix(1:4,2,2))
mat<-matrix(1:4,2,2)
mat
layout(mat)
layout.show(4)
layout(matrix(1:6,3,2))
layout.show(6)
layout(matrix(1:6,2,3))
layout.show(6)
m<-matrix(c(1:3,3),2,2)
layout(m)
layout.show(3)
m<-matrix(1:4,2,2)
layout(m,widths=c(1,3),heights=c(3,1))
layout.show(4)
m<-matrix(c(1,1,2,1),2,2)
layout(m,widths=c(2,1),heights=c(1,2))
layout.show(2)
m<-matrix(0:3,2,2)
layout(m,c(1,3),c(1,3))
layout.show(3)
devtools::build_vignettes()
devtools::document()
devtools::document()
library(StatComp18054)
help(package="StatComp18054")
install.packages("devtools")
devtools::install_github("zhzhy1/StatComp18054")
library(StatComp18054)
devtools::build_vignettes()
library(StatComp18054)
